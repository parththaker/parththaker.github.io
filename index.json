[{"authors":["admin"],"categories":null,"content":"I am working towards my PhD in Electrical Engineering department at Arizona State University. Currently, I am working with Prof. Gautam Dasarathy on the intersection between graphs and optimization.\nPreviously, I was holding the position of Systems Engineer at Netradyne . I was working on using sensor information from Mobile devices/IMU chips to draw out statistics on the general driving behavior of an individual to develop better driver safety features.\nI have dual degree (B.Tech + M.Tech) from Electrical Department at IIT Madras. I did my Senior thesis under the guidance of Dr. Radha Krishna Ganti on the broad topic Rank Preserving optimization using Factored Gradient Descent. My thesis work can be found here ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://parththaker.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am working towards my PhD in Electrical Engineering department at Arizona State University. Currently, I am working with Prof. Gautam Dasarathy on the intersection between graphs and optimization.\nPreviously, I was holding the position of Systems Engineer at Netradyne . I was working on using sensor information from Mobile devices/IMU chips to draw out statistics on the general driving behavior of an individual to develop better driver safety features.","tags":null,"title":"Parth K. Thaker","type":"authors"},{"authors":["Parth Thaker","Gautam Dasarathy","Angelia Nedich"],"categories":null,"content":"","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578960000,"objectID":"fe4559958c207e1b74ff9563778d23e6","permalink":"https://parththaker.github.io/publication/thaker-isit-2020/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/publication/thaker-isit-2020/","section":"publication","summary":"We consider the problem of recovering a complex vector from m quadratic measurements. This problem, known as quadratic feasibility, encompasses the well known phase retrieval problem and has applications in a wide range of important areas including power system state estimation and x-ray crystallography. In general, not only is the the quadratic feasibility problem NP-hard to solve, but it may in fact be unidentifiable. In this paper, we establish conditions under which this problem becomes identifiable, and further prove isometry properties in the case when the matrices are Hermitian matrices sampled from a complex Gaussian distribution. Moreover, we explore a nonconvex optimization formulation of this problem, and establish salient features of the associated optimization landscape that enables gradient algorithms with an arbitrary initialization to converge to a globally optimal point with a high probability. Our results also reveal sample complexity requirements for successfully identifying a feasible solution in these contexts.","tags":null,"title":"On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems","type":"publication"},{"authors":null,"categories":null,"content":"I was sitting in a guest lecture of Prof. Shankar Mahadevan from University of Massachusettes when he had come to my undergrad univ IIT Madras, where he was talking about his (at that time) recent work with his studnet Ian Gemp. He was stating intoduction of variational inequalties to a relatively clueless audience, introducing the age old methods of extragradient methods, etc.\nIn the middle he said somehtign which pinged my interest. He was saying about how Ruunge-Kutta methods method when used in Reinforcement learning to their problem seems to go on being more faster adn faster as we had mroe skippings/ more terms we were considering. THis was followed by a quesiton \u0026ldquo;So how far can we keep including these terms?\u0026rdquo;\nThis seemed sort of surreal to me. A system which just keeps accelerating towards perfection ? This needed more investigation.\nGlimpse of Ruunge_kutta Runge-Kutta method used to find approximate solutions in Ordinary Differential Equations seemed a lot close to the form of equations I was getting. (Maybe it\u0026rsquo;s the same \u0026hellip; and this all is obvious thing\u0026hellip; Not sure)\\\nA General Runge-Kutta Gradient descent looks like,\n$$\\begin{equation}k_1 = \\alpha\t\\nabla F(x_k)\\end{equation}$$ $$\\begin{equation}k_2 = \\alpha \\nabla F(x_k - a_{21}k_1)\\end{equation}$$ $$\\begin{equation}k_3 = \\alpha \\nabla F(x_k - a_{31}k_1 - a_{32}k_2)\\end{equation}$$\nSo for general term,\n$$\\begin{equation}k_s = \\alpha \\nabla F(x_k - a_{s1}k_1 - a_{s2}k_2 - \\dots - a_{s,s-1}k_{s-1})\\end{equation}$$\nThe next iterate $x_k$ in the sequence of the algorithm as,\n$$\\begin{equation}x_{k+1} = x_{k} - \\sum_{i=1}^sb_i k_i\\end{equation}$$\nConstrained Convex Optimization A simple constrained Optimization problem can be stated as, $$\\begin{equation} \\min\\ \\ \\ f(x)\\ \\ \\ \\ \\text{over}\\ \\ \\ \\ x\\in \\chi \\end{equation}$$\nwhere $f(x)$ is a doubly differentiable convex function. The normal gradient descent considering the steepest descent criteria would involve the following update equation,\n$$\\begin{equation} x_{k+1} = x_{k} - \\eta\\nabla f(x_k) \\end{equation}$$\nSince the next iterative point $x_{k+1}$ needs to belong to the constrained set $\\chi$, we have the Projection step as,\n$$\\begin{equation} x_{k+1} = P_{\\chi}(x_{k} - \\eta\\nabla f(x_k)) \\end{equation}$$\nwhere the projection operator $P_{\\chi}(x)$ is defined as, $$\\begin{equation} P_{\\chi}(x) = \\arg\\min_{y\\in \\chi}||x-y||^2 \\end{equation}$$\nWhat problem are we looking at here? In many constrained optimization problems, one of the major computation issue with projected gradient descent is computing the Projection of a candidate variable $x_k$. Hence, I am trying to reduce the total number of Projection computations required for the algorithm to converge.\nSolution? Taking the motivation from Ruunge-Kutta logic, consider the following iterations where we take projections on alternate steps as shown below. (I am calling it 2-step skip projection), $$\\begin{equation} y_{k+1} = x_{k} - \\eta_1\\nabla f(x_{k})\\end{equation}$$\n$$\\begin{equation} x_{k+1} = P_{\\chi}(y_{k+1}-\\eta_2\\nabla f(y_{k+1}))\\end{equation}$$\nSimilarly, we can consider a 3-step skip projection as well, $$\\begin{equation} y_{k+1} = x_{k} - \\eta_1\\nabla f(x_{k})\\end{equation}$$\n$$\\begin{equation} z_{k+1} = y_{k+1} - \\eta_2\\nabla f(y_{k+1})\\end{equation}$$\n$$\\begin{equation} x_{k+1} = P_{\\chi}(z_{k+1}-\\eta_3\\nabla f(z_{k+1}))\\end{equation}$$\nWe can keep increasing the number of skips. Plotting for a simple example,\nHere the each point in the graph represents the $error(k) = \\log (f(Pr(x_k)) - f(x^*))$.\nThe two extremes The two extreme can be thought of as follows:\n Standard Projected gradient descent  $$\\begin{equation} x_{k+1} = P_{\\chi}(x_{k} - \\eta\\nabla f(x_k)) \\end{equation}$$\nInfinite step look ahead gradient descent  This extreme case of this skipping will be when we are projecting only after we reach the optimal point $$\\begin{equation} y = \\arg\\min f(x) \\end{equation}$$\n$$\\begin{equation} x^* = P_{\\chi}(y) \\end{equation}$$\nThe problem with this being, the fixed point of both these cases ned not be the same and therein problem lies.\nBut this need not be the same as the fixed point of the iterative scheme, $$\\begin{equation} x_{k+1} = P_{\\chi}(x_{k} - \\eta\\nabla f(x_k)) \\end{equation}$$\nThus the error between this two schemes is what we are seeing in the Figure 2.\nPossibly Workaround We can make $\\eta$(stepsize) step dependent like $\\frac{1}{k}$ which follows $\\sum \\eta_k = \\infty$ and $\\sum \\eta^2 \u0026lt; \\infty$ which will again ensure that $error \\rightarrow 0$ as $k \\rightarrow \\infty$.\\\nUsing the same logic, I simulated it with the same functions as previous plots. Results are as follows,\nDrawbacks Even if this approach shows some change in the number of projections required, but this change wont be significant enough for a factor reduction. There are no significant change in the convergence rate of this new proposed algorithm, and hence I dont think there is much hope for the idea to form a good enough innovation.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1525176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525176000,"objectID":"944df5f45880ce0bb7ba84870a691371","permalink":"https://parththaker.github.io/post/skipping-projection/","publishdate":"2018-05-01T12:00:00Z","relpermalink":"/post/skipping-projection/","section":"post","summary":"Effects of skippings in projection for projected gradient descent. \n","tags":["projected gradient descent"],"title":"Skippings in projected gradient descent","type":"post"},{"authors":null,"categories":null,"content":"Though I myself am doubtful about a lot of explanations here. I\u0026rsquo;ll remember and explain whatever I can.\nLets first start with the basics, The chebyshev polynomial of degree $n$ is denoted by $T_n(x)$ and given by the formula,\n$$ \\begin{equation}T_n(x) = cos(n cos^{-1}(x))\\end{equation}$$\nThey form the recursive relation given by, $$T_n(x) = cos(n cos^{-1}(x))$$\n$$T_{n+1} (x) = 2 x T_n(x) - T_{n-1}$$\nExtrema of Chebyshev polynomial $T_n$ are given in the form of,\n$$ x_k = cos\\left(\\frac{2k-1}{2n}\\pi\\right),\\ \\ \\ \\ \\ k=1,2,\\dots, n$$\nThey look something like this\u0026hellip;\nthe extrema of Chebyshev polynomials are distributed over the entire range of the approximation and have alternating values of plus or minus unity. These characteristics make Chebyshev polynomials an ideal basis for approximating functions Its a well known fact that Chebyshev function approximation is as good as it gets.\nIt is known that the mean squared error for a function fitting is minimum for a minimax function. But evaluation of a minimax polynomial is computationally expensive. Chebyshev polynomial fitting is quite close to minimax polynomial fitting and is computationally much cheaper.\n$$f= \\sum_{i=0}^M c_iT_i$$\nwhere $T_i$ is the Chebyshev polynomial of order starting from i. {$c_i$} indicate the chebyshev coefficients. Normally the chebyshev coefficients falls off exponentially. Partly i used to think it was because that the chebyshev polynomials of order $i$ had a $2^i$ term as the coefficient of $x^i$. But the prof said that it was because of that the chebyshev coefficients dropping rate is directly proportional to the Region Of Convergence.\nIn order to apply stable Chebyshev fitting the Taylor expansion of the function should have a bounded reminder term. Other than this Chebyshev can be normally shrunk in the number of terms used for the polynomial fitting with a bounded error which is normally the sum of magnitudes of all the coefficients of the neglected terms. This is because the max value of the polynomials is +/- 1 and because the chebyshev coefficients die out much easily this error is bounded with a very small value when you neglect the higher order terms.\nA very good thing about chebyshev polynomails i liked was the intuition or a way to look at it. It can be looked as though it takes the points on the real line from -1 to 1 and maps them to a unit circle of radius 1 and center 0 and then back to the real line. If u think about it this mapping sort of elongates the function and makes the curves much less steeper. Thus it can be seen as to why the Chebyshev fitting requires much less order polynomial then its counter Taylor expansion, though both are polynomial fittings.\nWhen looked at the recurrence equation of chebyshev polynomials we have,\n$$T_{n+1} = 2xT_{n} -T_{n-1}$$\nAnalyzing this equation we get that the Chebyshev polynomial derived from this can only lead to stable solutions within +1 and -1. Analysis of the roots of characteristic equations makes it evident.\nChebyshev does better than Fourier in almost all cases except for periodic functions. Where Fourier can leave Cheby in dust. But both of them do worse for functions with non differentiable peaks or valleys within the interval of consideration. It is advised to break the functions at such peaks and then do Chebyshev fitting on separate parts for better results.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"c3cb6e349b02f6a76d388d43e4ea77e5","permalink":"https://parththaker.github.io/post/chebyshev/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/post/chebyshev/","section":"post","summary":"A introductory article over the construction and behavior of chebyshev polynomials\n","tags":["chebyshev","polynomials","numerical"],"title":"Basic look over Chebyshev Polynomials","type":"post"},{"authors":null,"categories":null,"content":"First I tried experimenting , first good/cool thing I found was this command :\ncat /dev/urandom | padsp tee /dev/audio \u0026gt; /dev/null  OR\ncat /dev/urandom | padsp tee /dev/dsp \u0026gt; /dev/null  Both produces white noise in speakers.\nOn good/sad thing is it sends to the audio sink you have\u0026hellip; If the comp is on speaker mode\u0026hellip; the noise will be in your speakers\nIf there are headphones, it will be your headphones..\nBut on trying to do something like this:\ncat \u0026lt;somefile\u0026gt;.mp3 | padsp tee /dev/audio \u0026gt; /dev/null  Unsurprisingly it didn\u0026rsquo;t work. Reason : My speculation, .mp3 is a coding format and it might not have suited the format in which /dev/audio wanted it\u0026hellip;\nOn to next cool thing (found a lot of them while tweaking things) is the program called mpg123. It is used as:\nmpg123 \u0026lt;somefile\u0026gt;.mp3 | /dev/pcsp  /dev/pcsp basically is your speaker. mpg123 decodes .mp3 file into raw streaming data which is piped (\u0026lsquo;|\u0026rsquo;) to speakers\nA good thing about speakers is that it can overlap all noises no locking system as to who can write and stuff. Nice one\u0026hellip;\nOk.. So we got a way in which we can run raw data streams into speakers. Now how about capturing what is being run to the speakers. In other words snooping on the /dev/pcsp (I am not sure /dev/pcsp is the right place to snoop.)\narecord -f cd -t raw  The above command starts recording whatever is heard on the speakers. Now to pipe it to a music file we use the \u0026lsquo;oggenc\u0026rsquo; package. So finally its\narecord -f cd -t raw | oggenc - -r -o file.ogg  This will create a music file in the current directory which is capturing all of the sound played in your speakers.\nSo where are we right now. We have a way to play to a speaker, we have a way to record from a speaker. Now the remaining is to make a connection.\nSupporting articles :\n  https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data    This article deals with piping raw data to files of 0 byte size (FIFO files) and then playing it.\n   https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife    This article deals with using netcat for listening and sending data on ports.\n License Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"ad77b7831a4b13e8e084520c7d377c01","permalink":"https://parththaker.github.io/post/working-with-speaker-port/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/post/working-with-speaker-port/","section":"post","summary":"A experiment journal entry for audio sink\n","tags":["audio sink","noise generation"],"title":"Experimentation with audio sink","type":"post"},{"authors":null,"categories":null,"content":"Linux DC++ is a quite popular File sharing application used on the Linux OS. It looks something like this …\nOne of the major benefits which I saw with Linux DC++ compared to its alternatives like Eiskalt DC++ is that Linux DC++ consumes a relatively smaller amount of RAM and doesn\u0026rsquo;t lag the system which I found to be a big drawback for Eiskalt DC++\nAnyway, there was one erratic behavior which I was quite irritated about in Linux DC++. At some random time on some random you are peacefully downloading stuff from DC and then suddenly the entire downloads directory is missing.\nYou already have space constraints in your system and then the downloads folder with some very ahem ahem important files just goes away and worse … the space is not even empty.\nSo started looking up the bug, and viola found it.\nYou can find all your stuff in folders within ~/.dc++/FileLists/. There will be a lot of folders with very weird names and all of your deleted stuff will be inside there. Just have to bring them out using the mv command.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"457797a023a713be60c535bb986dd2f8","permalink":"https://parththaker.github.io/post/caution-linux-dc/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/post/caution-linux-dc/","section":"post","summary":"A cautionary note to linux DC++ users\n","tags":["linux DC++","missing downloads"],"title":"Linux DC++ problem","type":"post"},{"authors":null,"categories":null,"content":"When you are dealing with multiple github accounts you have to be careful of two things:\n Having proper rights to pull from git Having proper rights to push to git  It may seem as to the solution to both the problems should be the same, but they are slightly different. Lets look at both the points :\nHaving proper rights to pull from git When you are dealing with multiple accounts, there is this nice post  which gives the instructions as to how to go about it.\nHere the basic essence of the whole procedure is that one has to be careful as to which SSH key is being associated with the github account. This is being precisely targeted in the above indicated post.\nHaving proper rights to push to git Now the first question which comes to mind is, \u0026ldquo;Why isn\u0026rsquo;t the last setup sufficient for resolving this?\u0026quot;.\nThat would have been the case (not requiring further complications) if only one user can work on a single SSH key. You can have a single SSH key being used by different user-names on different github project. Hence the question arises, \u0026ldquo;Which is the correct user-name for the current commit push?\u0026rdquo;\nThus, in conclusion, one should set up local config setting for each project using,\ngit config user.email \u0026quot;accountname@domain.com\u0026quot; git config user.name \u0026quot;username\u0026quot;  You can as well use the above commands with the global flag to set it common for all the projects as follows\ngit config --global user.email \u0026quot;accountname@domain.com\u0026quot; git config --global user.name \u0026quot;username\u0026quot;  This will set the credentials for user.email and user.name as a backup option when the local setting are not present. However I would discourage the use of the global flag. A logical thought experiment is below:\nLets consider you start a new project with a username which is different from what is set using the global flag. Now in case you forget to setup the local credentials and push your changes then the changes are being pushed with a undesirable username and email.\nIf on the other hand, the global user.email and user.name settings are not present, git throws error during pushing that you have not setup the local settings and thus cannot push, which will remind you to setup the credentials which you want to push the changes by.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"c094b5fe0f205da1067af3ee1a1408e0","permalink":"https://parththaker.github.io/post/github-managing-multiple-accounts/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/post/github-managing-multiple-accounts/","section":"post","summary":"A walkthrough on dealing with multiple github accounts\n","tags":["github","multiple accounts"],"title":"Managing multiple github accounts","type":"post"},{"authors":["Parth Thaker","Radha Krishna Ganti"],"categories":null,"content":"","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"72cfd28943304dbd0353731e90c10f46","permalink":"https://parththaker.github.io/publication/thaker-dual-thesis-2016/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/publication/thaker-dual-thesis-2016/","section":"publication","summary":"Recently, due to the increasing size of the data matrices, it has become more and more computationally expensive to operate of such matrices. We suggest some additional thoughts as to how to reduce this computational complexity. First, we suggest the possibility of cutting down on the number of projection operation in a normal constrained optimization setting and thereby cutting down on some of the vary taxing operations on matrices. We also indicate the benefits of limitations of this approach Second, we more to a specific notorious constraint i.e. rank constraint. We study the minimization of function $f(X)$ over the set of all $n * m$ matrices when we are supposed to satisfy a constraint rank=r. We propose a algorithm on the lines of Factored Gradient descent (FGD) and show its theoretical convergence and simulation results.","tags":null,"title":"Selected Topics in Constrained Optimization","type":"publication"},{"authors":["Parth Thaker","Aditya Gopalan","Rahul Vaze"],"categories":null,"content":"","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"360938d2a27c38c23b2b8596aee23257","permalink":"https://parththaker.github.io/publication/thaker-rawnet-2017/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/publication/thaker-rawnet-2017/","section":"publication","summary":"Motivated by applications in competitive WiFi sensing, and competition to grab user attention in social networks, the problem of when to arrive at/sample a shared resource/server platform with multiple players is considered. Server activity is intermittent, with the server switching between between ON and OFF periods alternatively. Each player spends a certain cost to sample the server state, and due of competition, the per-player service rate is inversely proportional to the number of connected/ arrived players. The objective of each player is to arrive/ sample the server as soon as any ON period begins while incurring minimal sensing cost and to avoid having many other players overlap in time with itself. For this competition model, we propose a distributed randomized learning algorithm (strategy to sample the server) for each player, which is shown to converge to a unique non-trivial fixed point. The fixed point is moreover shown to be a Nash equilibrium of a sensing game, where each player’s utility function is demonstrated to possess all the required selfishness tradeoffs","tags":null,"title":"When to arrive in a congested system : Achieving Equilibrium via Learning Algorithm","type":"publication"}]