[{"authors":["admin"],"categories":null,"content":"I am working towards my PhD in Electrical Engineering department at Arizona State University. Currently, I am working with Prof. Gautam Dasarathy on interesting problems at the intersection of Graph theory and Optimization.\nRecently, I interned at Mitsubishi Electric Research Laboratories (MERL) with Dr.Abraham P. Vinod where I developed bandit-based algorithms for resource monitoring. Previously, I was holding the position of Systems Engineer at Netradyne . I was working with sensor information from Mobile devices/IMU chips to draw out statistics on the general driving behavior of an individual to develop better driver safety features.\nI have dual degree (B.Tech + M.Tech) from Electrical Department at IIT Madras. I did my Senior thesis under the guidance of Dr. Radha Krishna Ganti on the broad topic of bi-level rank preserving algorithms. My thesis work can be found here ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://parththaker.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am working towards my PhD in Electrical Engineering department at Arizona State University. Currently, I am working with Prof. Gautam Dasarathy on interesting problems at the intersection of Graph theory and Optimization.\nRecently, I interned at Mitsubishi Electric Research Laboratories (MERL) with Dr.Abraham P. Vinod where I developed bandit-based algorithms for resource monitoring. Previously, I was holding the position of Systems Engineer at Netradyne . I was working with sensor information from Mobile devices/IMU chips to draw out statistics on the general driving behavior of an individual to develop better driver safety features.","tags":null,"title":"Parth K. Thaker","type":"authors"},{"authors":["Parth Thaker","Nikhil Rao","Mohit Malu","Gautam Dasarathy"],"categories":null,"content":"","date":1667865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667865600,"objectID":"dc164f928d874e5c25d10ba6a4e35c19","permalink":"https://parththaker.github.io/publication/thaker-graph-bandits-2021/","publishdate":"2022-11-08T00:00:00Z","relpermalink":"/publication/thaker-graph-bandits-2021/","section":"publication","summary":"We study pure exploration in multi armed bandits with graph side information. In particular, we consider the best-arm and near best-arm identification problem in the fixed confidence setting under the assumption that the arm rewards are smooth with respect to a given arbitrary graph. This captures a range of real world pure exploration scenarios where one often has information about the similarity of the options or actions under consideration. We propose a novel algorithm GRUB for this problem and provide a theoretical characterization of its performance that elicits the benefit of the graph side information. We complement our theory with experimental results that show that capitalizing on available graph side information yields significant improvements over pure exploration methods that are unable to use this information","tags":null,"title":"Maximizing and Satisficing and Multi-armed Bandits with Graph Information","type":"publication"},{"authors":["John Janiczek","Parth Thaker","Gautam Dasarathy","Christopher S. Edwards","Philip Christensen","Suren Jayasuriya"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"f616c25af9d79cb8e4601c0a1ec2f8d2","permalink":"https://parththaker.github.io/publication/john-thaker-differntial-programming-2020/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/john-thaker-differntial-programming-2020/","section":"publication","summary":" Hyperspectral unmixing is an important remote sensing task with applications including material identification and analysis. Characteristic spectral features make many pure materials identifiable from their visible-to-infrared spectra, but quantifying their presence within a mixture is a challenging task due to nonlinearities and factors of variation. In this paper, spectral variation is considered from a physics-based approach and incorporated into an end-to-end spectral unmixing algorithm via differentiable programming. The dispersion model is introduced to simulate realistic spectral variation, and an efficient method to fit the parameters is presented. Then, this dispersion model is utilized as a generative model within an analysis-by-synthesis spectral unmixing algorithm. Further, a technique for inverse rendering using a convolutional neural network to predict parameters of the generative model is introduced to enhance performance and speed when training data is available.","tags":null,"title":"Differentiable Programming for Hyperspectral Unmixing using a Physics-based Dispersion Model","type":"publication"},{"authors":null,"categories":null,"content":"During the course of attending Information Theory by Prof. Lalitha Sankar, I encountered the following question from Thomas \u0026amp; Clover in one of the homeworks:\nCalculate the capacity of the following channel with the probability transistion matrix,\n$$\\begin{bmatrix}p \u0026amp; 1-p \u0026amp; 0 \u0026amp; 0\\\\ 1-p \u0026amp; p \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; q \u0026amp; 1-q\\\\ 0 \u0026amp; 0 \u0026amp; 1-q \u0026amp; q \\end{bmatrix}$$\nMind\u0026rsquo;s muscle memory kicked in as the math derivation lead to the conclusion that optimal strategy is to use the channel according to the ratio: $\\frac{2^{-H(p)}}{2^{-H(p)} +2^{-H(q)}}$, where $H(\\alpha)$ would denote the entropy of Bernoulli($\\alpha$).\nA few moment of stagnant staring at the problem got me thinking of a potential use case.\nA student is attending a lecture and the BSC channels model the following\n First BSC channel : Denotes the capability of the student for understanding the material taught in the lecture. Let $X$ denote the event that the student is attentive to what is being taught in the lecture or not and $Y$ model whether the student understood the concept or not. $\\mathbb{P}(Y|X)$ denotes the probability matrix of the outcomes. In order for the channel to behave as a Bernoulli($\\alpha$), we make the following assumptions:  If the student is in tune with the teaching style of the lecturer, then $p\\approx 1$ (a perfect learner). $\\mathbb{P}(Y=1 | X=1) \\approx 1$ i.e. given complete attention, student understands it completely. On the other hand, given no attention student doesnt gain anything hence $P(Y=0 | X=0)\\approx 1$ If the student is completely oblivious to the lecturer\u0026rsquo;s teaching patterns and is attending the course because his grad advisor forced his hand, then we model it as a Bernoulli($\\frac{1}{2}$). This is because whether the student is attentive or not, the conclusion is that the student is not understanding the course content.    Second BSC channel : Denotes the capability of student in alternate pursuits in classroom \u0026ndash;  Thinking of research project the student is working on (this is me) Thinking of strategies in board games (also me. Terra Mystica !!!) Chatting on phone, replying to mails, twitter ,etc. The parameter $q$ models how clearly one can think of that other task being performed while in classroom.    If we consider this model then the surprising conclusion of this homework problem comes to :\nIt is information theoritically suboptimal to completely focus on the content being taught in class\nThere is an ideal ratio in which both needs to be pursued during your classroom time. So, the next time you have are caught distracted in class you have an argument to make.\nThough you cant completely zone out as well, since that is also a suboptimal strategy :)\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1598961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598961600,"objectID":"e03d593e5af0ec6f19589f23815b2761","permalink":"https://parththaker.github.io/post/parallel-bsc/","publishdate":"2020-09-01T12:00:00Z","relpermalink":"/post/parallel-bsc/","section":"post","summary":"A take on implication of information theoritic capacity on learning in classroom\n","tags":["Information theory","BSC channel","education"],"title":"Optimal strategy for classroom behaviour","type":"post"},{"authors":["Parth Thaker","Gautam Dasarathy","Angelia Nedich"],"categories":null,"content":"","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578960000,"objectID":"fe4559958c207e1b74ff9563778d23e6","permalink":"https://parththaker.github.io/publication/thaker-isit-2020/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/publication/thaker-isit-2020/","section":"publication","summary":"We consider the problem of recovering a complex vector from m quadratic measurements. This problem, known as quadratic feasibility, encompasses the well known phase retrieval problem and has applications in a wide range of important areas including power system state estimation and x-ray crystallography. In general, not only is the the quadratic feasibility problem NP-hard to solve, but it may in fact be unidentifiable. In this paper, we establish conditions under which this problem becomes identifiable, and further prove isometry properties in the case when the matrices are Hermitian matrices sampled from a complex Gaussian distribution. Moreover, we explore a nonconvex optimization formulation of this problem, and establish salient features of the associated optimization landscape that enables gradient algorithms with an arbitrary initialization to converge to a globally optimal point with a high probability. Our results also reveal sample complexity requirements for successfully identifying a feasible solution in these contexts.","tags":null,"title":"On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems","type":"publication"},{"authors":null,"categories":null,"content":"One of the major benefits (among million others) of attending an elite university like IIT Madras is the various avenues it provides to get a glimspe into a world which you never knew of. Pursuing research and the thought of PhD was one such world which was brought within my grasp by IITM.\nI was fortunate enough to attend one of the guest lecture by Prof. Sridhar Mahadevan , University of Massachusetts, during his visit to IIT Madras. He was going to discuss about his recent work (at that time) recent work with his student Ian Gemp . The plan of the talk (atleast to me) seemed to be : Start with variational inequalities and proceed to extragradient methods and state the results they worked out.\nIn the middle of the seminar, he said about a curious finding which pinged my interest. They had applied a logic based on Runge-Kutta methods to their problem and noticed that system kept converging faster and faster as the order of R-K method was increased. This was followed by a question by him,\nSo how much acceleration can we possibly obtain by including more and more terms?\nThis phenomenon irked me. According to my firm belief, there is always a trade-off. A system which just keeps providing improved one-step convergence without consequence? Something was definitely off.\nMy lifelong passion has been the theory behind optimization methods. Hence, I tried replicating similar situation (some would argue its exactly the same on some plane of thought) in constrained optimization using projected gradient descent.\nBefore diving deep into the exact question, a brief primer on the setup,\nConstrained Convex Optimization A simple constrained optimization problem of optimizing a convex function $f(x)$ over the set $x\\in\\chi$ can be stated as, $$\\begin{equation} \\min\\ \\ \\ f(x)\\ \\ \\ \\ \\text{over}\\ \\ \\ \\ x\\in \\chi \\end{equation}$$\nwhere assume that $f(x)$ is a doubly differentiable.\nOne of the go-to approach to solving the above problem is through projected gradient descent. The next iterate $x_{k+1}$ can we written as,\n$$\\begin{equation} x_{k+1} = P_{\\chi}(x_{k} - \\eta\\nabla f(x_k)) \\end{equation}$$\nwhere the projection operator $P_{\\chi}(x)$ is defined as, $$\\begin{equation} P_{\\chi}(x) = \\arg\\min_{y\\in \\chi}||x-y||^2 \\end{equation}$$\nTaking the que from Prof. Sridhar, consider the following iterations where we take projections on alternate steps as shown below. (I am calling it 2-step skip projection), $$\\begin{equation} y_{k+1} = x_{k} - \\eta_1\\nabla f(x_{k})\\end{equation}$$\n$$\\begin{equation} x_{k+1} = P_{\\chi}(y_{k+1}-\\eta_2\\nabla f(y_{k+1}))\\end{equation}$$\nSimilarly, we can consider a 3-step skip projection as well, $$\\begin{equation} y_{k+1} = x_{k} - \\eta_1\\nabla f(x_{k})\\end{equation}$$\n$$\\begin{equation} z_{k+1} = y_{k+1} - \\eta_2\\nabla f(y_{k+1})\\end{equation}$$\n$$\\begin{equation} x_{k+1} = P_{\\chi}(z_{k+1}-\\eta_3\\nabla f(z_{k+1}))\\end{equation}$$\nFurther, we can keep increasing the number of skips to obtain any $k$-skip projected gradient descent.\nBelow is the error plot of these $k$-skip projected gradient descent for a simple example, where $error(k) = \\log (f(Pr(x_k)) - f(x^*))$,\nInitial thoughts On first look, this experimental evidence confirms Prof.Sridhar\u0026rsquo;s claims!! More skipping just provides better one-step convergence.\nBut where is the trade-off? Lets look closely as to where the different $k$-skip algorithms are converging.\nNote after enlarging the tail of the graphs, we can notice at all the distinct $k$-skip algorithms have distinct convergence point. And moreover, the more order of skip -\u0026gt; the farther away from the optimal solution (here optimal solution is the convergence point of standard projected gradient descent)\nWhy are we observing this?\nThe two extreme behaviour We can consider the following high level view. Considering skipping with projected gradient descent, there are two extreme behaviour which can be thought of as follows:\n Standard Projected gradient descent :  $$\\begin{equation} x_{k+1} = P_{\\chi}(x_{k} - \\eta\\nabla f(x_k)) \\end{equation}$$\nInfinite-step look-ahead gradient descent projection : The extreme case of $k$-skip algorithms will be when we are projecting only after we reach the optimal point $$\\begin{equation} y = \\arg\\min f(x) \\end{equation}$$  $$\\begin{equation} x^* = P_{\\chi}(y) \\end{equation}$$ As noted in the sample simulations, the issue here is the difference in the fixed points in both these cases.\nThis provides a good intuition for the behaviour witnessed in tail-end of Figure 2.\nPossibly Workaround One easy and quick workaround to make sure that all the $k$-skip algorithms converge to the same point is to have decaying stepsizes i.e., we can make step-size (\\eta) decay as $\\frac{1}{t}$ which follows $\\sum \\eta_t = \\infty$ and $\\sum \\eta_t^2 \u0026lt; \\infty$.\nSimulating the same setup with the new step-size routine, we obtain the following tail behaviour,\nBenefits In constrained optimization, one of the major computation issues concerning projected gradient descent is the projection step. Using the $k$-skip projected gradient descent reduces the total number of projections required by a multiplicative factor.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1525176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525176000,"objectID":"944df5f45880ce0bb7ba84870a691371","permalink":"https://parththaker.github.io/post/skipping-projection/","publishdate":"2018-05-01T12:00:00Z","relpermalink":"/post/skipping-projection/","section":"post","summary":"Effects of skipping the projection for gradient descent with projections. \n","tags":["projected gradient descent"],"title":"Projected gradient descent with skipping","type":"post"},{"authors":null,"categories":null,"content":"First I tried experimenting , first good/cool thing I found was this command :\ncat /dev/urandom | padsp tee /dev/audio \u0026gt; /dev/null  OR\ncat /dev/urandom | padsp tee /dev/dsp \u0026gt; /dev/null  Both produces white noise in speakers.\nOn good/sad thing is it sends to the audio sink you have\u0026hellip; If the comp is on speaker mode\u0026hellip; the noise will be in your speakers\nIf there are headphones, it will be your headphones..\nBut on trying to do something like this:\ncat \u0026lt;somefile\u0026gt;.mp3 | padsp tee /dev/audio \u0026gt; /dev/null  Unsurprisingly it didn\u0026rsquo;t work. Reason : My speculation, .mp3 is a coding format and it might not have suited the format in which /dev/audio wanted it\u0026hellip;\nOn to next cool thing (found a lot of them while tweaking things) is the program called mpg123. It is used as:\nmpg123 \u0026lt;somefile\u0026gt;.mp3 | /dev/pcsp  /dev/pcsp basically is your speaker. mpg123 decodes .mp3 file into raw streaming data which is piped (\u0026lsquo;|\u0026rsquo;) to speakers\nA good thing about speakers is that it can overlap all noises no locking system as to who can write and stuff. Nice one\u0026hellip;\nOk.. So we got a way in which we can run raw data streams into speakers. Now how about capturing what is being run to the speakers. In other words snooping on the /dev/pcsp (I am not sure /dev/pcsp is the right place to snoop.)\narecord -f cd -t raw  The above command starts recording whatever is heard on the speakers. Now to pipe it to a music file we use the \u0026lsquo;oggenc\u0026rsquo; package. So finally its\narecord -f cd -t raw | oggenc - -r -o file.ogg  This will create a music file in the current directory which is capturing all of the sound played in your speakers.\nSo where are we right now. We have a way to play to a speaker, we have a way to record from a speaker. Now the remaining is to make a connection.\nSupporting articles :\n  https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data    This article deals with piping raw data to files of 0 byte size (FIFO files) and then playing it.\n   https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife    This article deals with using netcat for listening and sending data on ports.\n License Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1501588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501588800,"objectID":"65e75f03f46f9ab1aa3da03edbe16e25","permalink":"https://parththaker.github.io/draft_posts/working-with-speaker-port/","publishdate":"2017-08-01T12:00:00Z","relpermalink":"/draft_posts/working-with-speaker-port/","section":"draft_posts","summary":"A experiment journal entry for audio sink\n","tags":["audio sink","noise generation"],"title":"Experimentation with audio sink","type":"draft_posts"},{"authors":null,"categories":null,"content":"Linux DC++ is a quite popular File sharing application used on the Linux OS. It looks something like this …\nOne of the major benefits which I saw with Linux DC++ compared to its alternatives like Eiskalt DC++ is that Linux DC++ consumes a relatively smaller amount of RAM and doesn\u0026rsquo;t lag the system which I found to be a big drawback for Eiskalt DC++\nAnyway, there was one erratic behavior which I was quite irritated about in Linux DC++. At some random time on some random you are peacefully downloading stuff from DC and then suddenly the entire downloads directory is missing.\nYou already have space constraints in your system and then the downloads folder with some very ahem ahem important files just go away and worse … the space is not even empty.\nSo I started looking up the bug, and viola found it.\nYou can find all your stuff in folders within ~/.dc++/FileLists/. There will be a lot of folders with very weird names and all of your deleted stuff will be inside there. Just have to bring them out using the mv command.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"4f93a9df079edabde4dd67f7b2deca45","permalink":"https://parththaker.github.io/draft_posts/caution-linux-dc/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/draft_posts/caution-linux-dc/","section":"draft_posts","summary":"A cautionary note to linux DC++ users\n","tags":["linux DC++","missing downloads"],"title":"Linux DC++ problem","type":"draft_posts"},{"authors":null,"categories":null,"content":"When you are dealing with multiple github accounts you have to be careful of two things:\n Having proper rights to pull from git Having proper rights to push to git  It may seem as to the solution to both the problems should be the same, but they are slightly different. Lets look at both the points :\nHaving proper rights to pull from git When you are dealing with multiple accounts, there is this nice post  which gives the instructions as to how to go about it.\nHere the basic essence of the whole procedure is that one has to be careful as to which SSH key is being associated with the github account. This is being precisely targeted in the above indicated post.\nHaving proper rights to push to git Now the first question which comes to mind is, \u0026ldquo;Why isn\u0026rsquo;t the last setup sufficient for resolving this?\u0026quot;.\nThat would have been the case (not requiring further complications) if only one user can work on a single SSH key. You can have a single SSH key being used by different user-names on different github project. Hence the question arises, \u0026ldquo;Which is the correct user-name for the current commit push?\u0026rdquo;\nThus, in conclusion, one should set up local config setting for each project using,\ngit config user.email \u0026quot;accountname@domain.com\u0026quot; git config user.name \u0026quot;username\u0026quot;  You can as well use the above commands with the global flag to set it common for all the projects as follows\ngit config --global user.email \u0026quot;accountname@domain.com\u0026quot; git config --global user.name \u0026quot;username\u0026quot;  This will set the credentials for user.email and user.name as a backup option when the local setting are not present. However I would discourage the use of the global flag. A logical thought experiment is below:\nLets consider you start a new project with a username which is different from what is set using the global flag. Now in case you forget to setup the local credentials and push your changes then the changes are being pushed with a undesirable username and email.\nIf on the other hand, the global user.email and user.name settings are not present, git throws error during pushing that you have not setup the local settings and thus cannot push, which will remind you to setup the credentials which you want to push the changes by.\nLicense Copyright 2017 Parth Thaker .\nReleased under the MIT license.\n","date":1498910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498910400,"objectID":"f7999209e4e78b42693c85e0b0db95a1","permalink":"https://parththaker.github.io/draft_posts/github-managing-multiple-accounts/","publishdate":"2017-07-01T12:00:00Z","relpermalink":"/draft_posts/github-managing-multiple-accounts/","section":"draft_posts","summary":"A walkthrough on dealing with multiple github accounts\n","tags":["github","multiple accounts"],"title":"Managing multiple github accounts","type":"draft_posts"},{"authors":["Parth Thaker","Radha Krishna Ganti"],"categories":null,"content":"","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"72cfd28943304dbd0353731e90c10f46","permalink":"https://parththaker.github.io/publication/thaker-dual-thesis-2016/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/publication/thaker-dual-thesis-2016/","section":"publication","summary":"We study the problem of minimization of loss function $f(X)$ over the set of all $n$ x $m$ matrices $X$ of rank $r$. We propose a bi-level extention to Factored Gradient descent (FGD) algorithm and show its theoretical convergence and simulation results. On a side note, we also suggest methods of reducing computation complexity of projection based algorithms by cutting down on the number of projection operation in a standard constrained optimization approach. We also analyse the benefits and limitations of this approach.","tags":null,"title":"Selected Topics in Constrained Optimization","type":"publication"},{"authors":["Parth Thaker","Aditya Gopalan","Rahul Vaze"],"categories":null,"content":"","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"360938d2a27c38c23b2b8596aee23257","permalink":"https://parththaker.github.io/publication/thaker-rawnet-2017/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/publication/thaker-rawnet-2017/","section":"publication","summary":"Motivated by applications in competitive WiFi sensing, and competition to grab user attention in social networks, the problem of when to arrive at/sample a shared resource/server platform with multiple players is considered. Server activity is intermittent, with the server switching between between ON and OFF periods alternatively. Each player spends a certain cost to sample the server state, and due of competition, the per-player service rate is inversely proportional to the number of connected/ arrived players. The objective of each player is to arrive/ sample the server as soon as any ON period begins while incurring minimal sensing cost and to avoid having many other players overlap in time with itself. For this competition model, we propose a distributed randomized learning algorithm (strategy to sample the server) for each player, which is shown to converge to a unique non-trivial fixed point. The fixed point is moreover shown to be a Nash equilibrium of a sensing game, where each player’s utility function is demonstrated to possess all the required selfishness tradeoffs","tags":null,"title":"When to arrive in a congested system : Achieving Equilibrium via Learning Algorithm","type":"publication"}]