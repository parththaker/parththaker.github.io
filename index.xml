<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parth Thaker</title>
    <link>https://parththaker.github.io/</link>
      <atom:link href="https://parththaker.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Parth Thaker</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2022 Parth Thaker</copyright><lastBuildDate>Tue, 08 Nov 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Parth Thaker</title>
      <link>https://parththaker.github.io/</link>
    </image>
    
    <item>
      <title>Maximizing and Satisficing and Multi-armed Bandits with Graph Information</title>
      <link>https://parththaker.github.io/publication/thaker-graph-bandits-2021/</link>
      <pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-graph-bandits-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentiable Programming for Hyperspectral Unmixing using a Physics-based Dispersion Model</title>
      <link>https://parththaker.github.io/publication/john-thaker-differntial-programming-2020/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/john-thaker-differntial-programming-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimal strategy for classroom behaviour</title>
      <link>https://parththaker.github.io/post/parallel-bsc/</link>
      <pubDate>Tue, 01 Sep 2020 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/parallel-bsc/</guid>
      <description>&lt;p&gt;During the course of attending Information Theory by Prof. Lalitha Sankar, I encountered the following question from Thomas &amp;amp; Clover in one of the homeworks:&lt;/p&gt;
&lt;p&gt;Calculate the capacity of the following channel with the probability transistion matrix,&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix}p &amp;amp; 1-p &amp;amp; 0 &amp;amp; 0\\ 1-p &amp;amp; p &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; q &amp;amp; 1-q\\ 0 &amp;amp; 0 &amp;amp; 1-q &amp;amp; q \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Mind&amp;rsquo;s muscle memory kicked in as the math derivation lead to the conclusion that optimal strategy is to use the channel according to the ratio: $\frac{2^{-H(p)}}{2^{-H(p)} +2^{-H(q)}}$, where $H(\alpha)$ would denote the entropy of Bernoulli($\alpha$).&lt;/p&gt;
&lt;p&gt;A few moment of stagnant staring at the problem got me thinking of a potential use case.&lt;/p&gt;
&lt;p&gt;A student is attending a lecture and the BSC channels model the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First BSC channel&lt;/strong&gt; : Denotes the capability of the student for understanding the material taught in the lecture. Let $X$ denote the event that the student is attentive to what is being taught in the lecture or not and $Y$ model whether the student understood the concept or not.
$\mathbb{P}(Y|X)$ denotes the probability matrix of the outcomes.
In order for the channel to behave as a Bernoulli($\alpha$), we make the following assumptions:
&lt;ul&gt;
&lt;li&gt;If the student is in tune with the teaching style of the lecturer, then $p\approx 1$ (a perfect learner). $\mathbb{P}(Y=1 | X=1) \approx 1$ i.e. given complete attention, student understands it completely. On the other hand, given no attention student doesnt gain anything hence $P(Y=0 | X=0)\approx 1$&lt;/li&gt;
&lt;li&gt;If the student is completely oblivious to the lecturer&amp;rsquo;s teaching patterns and is attending the course because his grad advisor forced his hand, then we model it as a Bernoulli($\frac{1}{2}$). This is because whether the student is attentive or not, the conclusion is that the student is not understanding the course content.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!--the of the material being taught in class -- By this I mean, for a person having full knowledge of measure theory might find an introductory class on Probability a bit easy. 
Thus the person would have parameter $p\approx \frac{1}{2}$, which indicates the corresponds that channel being noisy $\leftrightarrow$ easy class. This would also mean that a look at the material taught by the prof once every 10 minutes would give him a complete picture of what is happening in class. 
--&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Second BSC channel&lt;/strong&gt; : Denotes the capability of student in alternate pursuits in classroom &amp;ndash;
&lt;ul&gt;
&lt;li&gt;Thinking of research project the student is working on (this is me)&lt;/li&gt;
&lt;li&gt;Thinking of strategies in board games (also me. 
&lt;a href=&#34;https://boardgamegeek.com/boardgame/120677/terra-mystica&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Terra Mystica&lt;/a&gt;
!!!)&lt;/li&gt;
&lt;li&gt;Chatting on phone, replying to mails, twitter ,etc.
The parameter $q$ models how clearly one can think of that other task being performed while in classroom.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we consider this model then the surprising conclusion of this homework problem comes to :&lt;/p&gt;
&lt;p&gt;&lt;em&gt;It is information theoritically suboptimal to completely focus on the content being taught in class&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is an ideal ratio in which both needs to be pursued during your classroom time. So, the next time you have are caught distracted in class you have an argument to make.&lt;/p&gt;
&lt;p&gt;Though you cant completely zone out as well, since that is also a suboptimal strategy :)&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems</title>
      <link>https://parththaker.github.io/publication/thaker-isit-2020/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-isit-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projected gradient descent with skipping</title>
      <link>https://parththaker.github.io/post/skipping-projection/</link>
      <pubDate>Tue, 01 May 2018 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/skipping-projection/</guid>
      <description>&lt;p&gt;One of the major benefits (among million others) of attending an elite university like IIT Madras is the various avenues it provides to get a glimspe into a world which you never knew of. Pursuing research and the thought of PhD was one such world which was brought within my grasp by IITM.&lt;/p&gt;
&lt;p&gt;I was fortunate enough to attend one of the guest lecture by 
&lt;a href=&#34;http://people.cs.umass.edu/~mahadeva/Site/About_Me.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Sridhar Mahadevan&lt;/a&gt;
, University of Massachusetts, during his visit to IIT Madras.
He was going to discuss about his recent work (at that time) 
&lt;a href=&#34;https://arxiv.org/pdf/1405.6757v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent work&lt;/a&gt;
 with his student 
&lt;a href=&#34;https://imgemp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian Gemp&lt;/a&gt;
.
The plan of the talk (atleast to me) seemed to be : Start with variational inequalities and proceed to extragradient methods and state the results they worked out.&lt;/p&gt;
&lt;p&gt;In the middle of the seminar, he said about a curious finding which pinged my interest.
They had applied a logic based on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runge-Kutta methods&lt;/a&gt;
 to their problem and noticed that system kept converging faster and faster as the order of R-K method was increased.
This was followed by a question by him,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;So how much acceleration can we possibly obtain by including more and more terms?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This phenomenon irked me. According to my firm belief, there is &lt;strong&gt;always&lt;/strong&gt; a trade-off. A system which just keeps providing improved one-step convergence without consequence? Something was definitely off.&lt;/p&gt;
&lt;!--
## Glimpse of Runge-kutta

Runge-Kutta methods have been used to find approximate solutions in Ordinary Differential Equations.

A General Runge-Kutta Gradient descent update looks like,

$$\begin{equation}k_1 = \alpha  \nabla F(x_k)\end{equation}$$
$$\begin{equation}k_2 = \alpha \nabla F(x_k - a\_{21}k_1)\end{equation}$$
$$\begin{equation}k_3 = \alpha \nabla F(x_k - a\_{31}k_1 - a\_{32}k_2)\end{equation}$$

So for general term,

$$\begin{equation}k_s = \alpha \nabla F(x_k - a\_{s1}k_1 - a\_{s2}k_2 - \dots - a\_{s,s-1}k\_{s-1})\end{equation}$$

The next iterate $x_k$ in the sequence of the algorithm as,

$$\begin{equation}x\_{k+1} = x\_{k} - \sum\_{i=1}^sb_i k_i\end{equation}$$
--&gt;
&lt;p&gt;My lifelong passion has been the theory behind optimization methods. Hence, I tried replicating similar situation (some would argue its exactly the same on some plane of thought) in constrained optimization using projected gradient descent.&lt;/p&gt;
&lt;p&gt;Before diving deep into the exact question, a brief primer on the setup,&lt;/p&gt;
&lt;h2 id=&#34;constrained-convex-optimization&#34;&gt;Constrained Convex Optimization&lt;/h2&gt;
&lt;p&gt;A simple constrained optimization problem of optimizing a convex function $f(x)$ over the set $x\in\chi$ can be stated as,
$$\begin{equation}
\min\ \ \ f(x)\ \ \ \  \text{over}\ \ \ \  x\in \chi
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where assume that $f(x)$ is a doubly differentiable.&lt;/p&gt;
&lt;p&gt;One of the go-to approach to solving the above problem is through projected gradient descent.
The next iterate $x_{k+1}$ can we written as,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where the projection operator $P_{\chi}(x)$ is defined as,
$$\begin{equation}
P_{\chi}(x) = \arg\min_{y\in \chi}||x-y||^2
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Taking the que from Prof. Sridhar, consider the following iterations where we take projections on alternate steps as shown below. (I am calling it 2-step skip projection),
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(y_{k+1}-\eta_2\nabla f(y_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;Similarly, we can consider a 3-step skip projection as well,
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
z_{k+1} = y_{k+1} - \eta_2\nabla f(y_{k+1})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(z_{k+1}-\eta_3\nabla f(z_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;Further, we can keep increasing the number of skips to obtain any $k$-skip projected gradient descent.&lt;/p&gt;
&lt;p&gt;Below is the error plot of these $k$-skip projected gradient descent for a simple example, where $error(k) = \log (f(Pr(x_k)) - f(x^*))$,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_overview.png&#34; alt=&#34; Overview &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;initial-thoughts&#34;&gt;Initial thoughts&lt;/h2&gt;
&lt;p&gt;On first look, this experimental evidence confirms Prof.Sridhar&amp;rsquo;s claims!! More skipping just provides better one-step convergence.&lt;/p&gt;
&lt;p&gt;But where is the trade-off? Lets look closely as to &lt;em&gt;where&lt;/em&gt; the different $k$-skip algorithms are converging.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_zoom.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;p&gt;Note after enlarging the tail of the graphs, we can notice at all the distinct $k$-skip algorithms have &lt;em&gt;distinct&lt;/em&gt; convergence point.
And moreover, the more order of skip -&amp;gt; the farther away from the optimal solution (here optimal solution is the convergence point of standard projected gradient descent)&lt;/p&gt;
&lt;p&gt;Why are we observing this?&lt;/p&gt;
&lt;h2 id=&#34;the-two-extreme-behaviour&#34;&gt;The two extreme behaviour&lt;/h2&gt;
&lt;p&gt;We can consider the following high level view. Considering skipping with projected gradient descent, there are two extreme behaviour which can be thought of as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Standard Projected gradient descent&lt;/strong&gt; :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Infinite-step look-ahead gradient descent projection&lt;/strong&gt; :
The extreme case of $k$-skip algorithms will be when we are projecting only after we reach the optimal point
$$\begin{equation}
y = \arg\min f(x)
\end{equation}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{equation}
x^* = P_{\chi}(y)
\end{equation}$$
As noted in the sample simulations, the issue here is the difference in the fixed points in both these cases.&lt;/p&gt;
&lt;p&gt;This provides a good intuition for the behaviour witnessed in tail-end of Figure 2.&lt;/p&gt;
&lt;h2 id=&#34;possibly-workaround&#34;&gt;Possibly Workaround&lt;/h2&gt;
&lt;p&gt;One easy and quick workaround to make sure that all the $k$-skip algorithms converge to the &lt;em&gt;same&lt;/em&gt; point is to have decaying stepsizes i.e., we can make step-size (\eta) decay as $\frac{1}{t}$ which follows $\sum \eta_t = \infty$ and $\sum \eta_t^2 &amp;lt; \infty$.&lt;/p&gt;
&lt;p&gt;Simulating the same setup with the new step-size routine, we obtain the following tail behaviour,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_decay.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;benefits&#34;&gt;Benefits&lt;/h2&gt;
&lt;p&gt;In constrained optimization, one of the major computation issues concerning projected gradient descent is the projection step. Using the $k$-skip projected gradient descent reduces the total number of projections required by a multiplicative factor.&lt;/p&gt;
&lt;!--
## Drawbacks

Even if this approach shows some change in the number of projections required, but this change wont be significant enough for a factor reduction.
There are no significant changes in the convergence rate of this new proposed algorithm, and hence I dont think there is much hope for the idea to form a good enough innovation.
--&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experimentation with audio sink</title>
      <link>https://parththaker.github.io/draft_posts/working-with-speaker-port/</link>
      <pubDate>Tue, 01 Aug 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/draft_posts/working-with-speaker-port/</guid>
      <description>&lt;p&gt;First I tried experimenting , first good/cool thing I found was this command :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /dev/urandom | padsp tee /dev/audio &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OR&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /dev/urandom | padsp tee /dev/dsp &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both produces white noise in speakers.&lt;/p&gt;
&lt;p&gt;On good/sad thing is it sends to the audio sink you have&amp;hellip; If the comp is on speaker mode&amp;hellip; the noise will be in your speakers&lt;/p&gt;
&lt;p&gt;If there are headphones, it will be your headphones..&lt;/p&gt;
&lt;p&gt;But on trying to do something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;somefile&amp;gt;.mp3 | padsp tee /dev/audio &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly it didn&amp;rsquo;t work. Reason : My speculation, .mp3 is a coding format and it might not have suited the format in which /dev/audio wanted it&amp;hellip;&lt;/p&gt;
&lt;p&gt;On to next cool thing (found a lot of them while tweaking things) is the program called mpg123. It is used as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpg123 &amp;lt;somefile&amp;gt;.mp3 | /dev/pcsp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;/dev/pcsp&lt;/code&gt; basically is your speaker. &lt;code&gt;mpg123&lt;/code&gt; decodes .mp3 file into raw streaming data which is piped (&amp;lsquo;|&amp;rsquo;) to speakers&lt;/p&gt;
&lt;p&gt;A good thing about speakers is that it can overlap all noises no locking system as to who can write and stuff. Nice one&amp;hellip;&lt;/p&gt;
&lt;p&gt;Ok.. So we got a way in which we can run raw data streams into speakers. Now how about capturing what is being run to the speakers. In other words snooping on the /dev/pcsp (I am not sure /dev/pcsp is the right place to snoop.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;arecord -f cd -t raw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above command starts recording whatever is heard on the speakers. Now to pipe it to a music file we use the &amp;lsquo;oggenc&amp;rsquo; package. So finally its&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;arecord -f cd -t raw | oggenc - -r -o file.ogg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a music file in the current directory which is capturing all of the sound played in your speakers.&lt;/p&gt;
&lt;p&gt;So where are we right now. We have a way to play to a speaker, we have a way to record from a speaker. Now the remaining is to make a connection.&lt;/p&gt;
&lt;p&gt;Supporting articles :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This article deals with piping raw data to files of 0 byte size (FIFO files) and then playing it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This article deals with using &lt;code&gt;netcat&lt;/code&gt; for listening and sending data on ports.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux DC&#43;&#43; problem</title>
      <link>https://parththaker.github.io/draft_posts/caution-linux-dc/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/draft_posts/caution-linux-dc/</guid>
      <description>&lt;p&gt;Linux DC++ is a quite popular File sharing application used on the Linux OS. It looks something like this â¦&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/linuxdc.png&#34; alt=&#34;LinuxDC++&#34;&gt;&lt;/p&gt;
&lt;p&gt;One of the major benefits which I saw with Linux DC++ compared to its alternatives like Eiskalt DC++ is that Linux DC++ consumes a relatively smaller amount of RAM and doesn&amp;rsquo;t lag the system which I found to be a big drawback for Eiskalt DC++&lt;/p&gt;
&lt;p&gt;Anyway, there was one erratic behavior which I was quite irritated about in Linux DC++. At some random time on some random you are peacefully downloading stuff from DC and then suddenly the entire downloads directory is missing.&lt;/p&gt;
&lt;p&gt;You already have space constraints in your system and then the downloads folder with some very &lt;em&gt;ahem ahem&lt;/em&gt; important files just go away and worse â¦ the space is not even empty.&lt;/p&gt;
&lt;p&gt;So I started looking up the bug, and viola found it.&lt;/p&gt;
&lt;p&gt;You can find all your stuff in folders within ~/.dc++/FileLists/. There will be a lot of folders with very weird names and all of your deleted stuff will be inside there. Just have to bring them out using the mv command.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing multiple github accounts</title>
      <link>https://parththaker.github.io/draft_posts/github-managing-multiple-accounts/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/draft_posts/github-managing-multiple-accounts/</guid>
      <description>&lt;p&gt;When you are dealing with multiple github accounts you have to be careful of two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having proper rights to pull from git&lt;/li&gt;
&lt;li&gt;Having proper rights to push to git&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It may seem as to the solution to both the problems should be the same, but they are slightly different. Lets look at both the points :&lt;/p&gt;
&lt;h2 id=&#34;having-proper-rights-to-pull-from-git&#34;&gt;Having proper rights to pull from git&lt;/h2&gt;
&lt;p&gt;When you are dealing with multiple accounts, there is 
&lt;a href=&#34;http://mherman.org/blog/2013/09/16/managing-multiple-github-accounts/#.WFKRwHV948o&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; this nice post &lt;/a&gt;
 which gives the instructions as to how to go about it.&lt;/p&gt;
&lt;p&gt;Here the basic essence of the whole procedure is that one has to be careful as to which SSH key is being associated with the github account. This is being precisely targeted in the above indicated post.&lt;/p&gt;
&lt;h2 id=&#34;having-proper-rights-to-push-to-git&#34;&gt;Having proper rights to push to git&lt;/h2&gt;
&lt;p&gt;Now the first question which comes to mind is, &amp;ldquo;Why isn&amp;rsquo;t the last setup sufficient for resolving this?&amp;quot;.&lt;/p&gt;
&lt;p&gt;That would have been the case (not requiring further complications) if only one user can work on a single SSH key. You can have a single SSH key being used by different user-names on different github project. Hence the question arises, &amp;ldquo;Which is the correct user-name for the current commit push?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Thus, in conclusion, one should set up local config setting for each project using,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git config user.email &amp;quot;accountname@domain.com&amp;quot;
git config user.name &amp;quot;username&amp;quot;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can as well use the above commands with the global flag to set it common for all the projects as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git config --global user.email &amp;quot;accountname@domain.com&amp;quot;
git config --global user.name &amp;quot;username&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will set the credentials for user.email and user.name as a backup option when the local setting are not present. However I would discourage the use of the global flag. A logical thought experiment is below:&lt;/p&gt;
&lt;p&gt;Lets consider you start a new project with a username which is different from what is set using the global flag. Now in case you forget to setup the local credentials and push your changes then the changes are being pushed with a undesirable username and email.&lt;/p&gt;
&lt;p&gt;If on the other hand, the global user.email and user.name settings are not present, git throws error during pushing that you have not setup the local settings and thus cannot push, which will remind you to setup the credentials which you want to push the changes by.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selected Topics in Constrained Optimization</title>
      <link>https://parththaker.github.io/publication/thaker-dual-thesis-2016/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-dual-thesis-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When to arrive in a congested system : Achieving Equilibrium via Learning Algorithm</title>
      <link>https://parththaker.github.io/publication/thaker-rawnet-2017/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-rawnet-2017/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
