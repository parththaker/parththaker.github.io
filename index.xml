<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parth Thaker</title>
    <link>https://parththaker.github.io/</link>
      <atom:link href="https://parththaker.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Parth Thaker</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Parth Thaker</copyright><lastBuildDate>Tue, 14 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Parth Thaker</title>
      <link>https://parththaker.github.io/</link>
    </image>
    
    <item>
      <title>On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems</title>
      <link>https://parththaker.github.io/publication/thaker-isit-2020/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-isit-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Skippings in projected gradient descent</title>
      <link>https://parththaker.github.io/post/skipping-projection/</link>
      <pubDate>Tue, 01 May 2018 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/skipping-projection/</guid>
      <description>&lt;p&gt;I was sitting in a guest lecture of 
&lt;a href=&#34;http://people.cs.umass.edu/~mahadeva/Site/About_Me.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Sridhar Mahadevan&lt;/a&gt;
, University of Massachusetts, when he visited my undergrad university IIT Madras.
He was talking about his (at that time) 
&lt;a href=&#34;https://arxiv.org/pdf/1405.6757v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent work&lt;/a&gt;
 with his student Ian Gemp.
He was stating the introduction of variational inequalities to a relatively clueless audience, introducing the age old extragradient methods, etc.&lt;/p&gt;
&lt;p&gt;In the middle of the session, he said something which pinged my interest.
He was explaining about how 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runge-Kutta methods&lt;/a&gt;
 method, used in Reinforcement learning , was applied to their problem. This method seems to go on being faster and faster as we had more skippings/more terms per update were being considered.
This was followed by a question &amp;ldquo;So how far can we keep including these terms?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;This seemed sort of surreal to me. A system which just keeps accelerating towards perfection ? This needed more investigation.&lt;/p&gt;
&lt;h2 id=&#34;glimpse-of-ruunge_kutta&#34;&gt;Glimpse of Ruunge_kutta&lt;/h2&gt;
&lt;p&gt;Runge-Kutta methods have been used to find approximate solutions in Ordinary Differential Equations.&lt;/p&gt;
&lt;p&gt;A General Runge-Kutta Gradient descent update looks like,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}k_1 = \alpha  \nabla F(x_k)\end{equation}$$
$$\begin{equation}k_2 = \alpha \nabla F(x_k - a_{21}k_1)\end{equation}$$
$$\begin{equation}k_3 = \alpha \nabla F(x_k - a_{31}k_1 - a_{32}k_2)\end{equation}$$&lt;/p&gt;
&lt;p&gt;So for general term,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}k_s = \alpha \nabla F(x_k - a_{s1}k_1 - a_{s2}k_2 - \dots - a_{s,s-1}k_{s-1})\end{equation}$$&lt;/p&gt;
&lt;p&gt;The next iterate $x_k$ in the sequence of the algorithm as,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}x_{k+1} = x_{k} - \sum_{i=1}^sb_i k_i\end{equation}$$&lt;/p&gt;
&lt;h2 id=&#34;constrained-convex-optimization&#34;&gt;Constrained Convex Optimization&lt;/h2&gt;
&lt;p&gt;A simple constrained Optimization problem can be stated as,
$$\begin{equation}
\min\ \ \ f(x)\ \ \ \  \text{over}\ \ \ \  x\in \chi
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where $f(x)$ is a doubly differentiable convex function. The normal gradient descent considering the steepest descent criteria would involve the following update equation,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = x_{k} - \eta\nabla f(x_k)
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Since the next iterative point $x_{k+1}$ needs to be in the constrained set $\chi$, we have the projection step as,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where the projection operator $P_{\chi}(x)$ is defined as,
$$\begin{equation}
P_{\chi}(x) = \arg\min_{y\in \chi}||x-y||^2
\end{equation}$$&lt;/p&gt;
&lt;h3 id=&#34;what-problem-are-we-looking-at-here&#34;&gt;What problem are we looking at here?&lt;/h3&gt;
&lt;p&gt;In many constrained optimization problems, one of the major computation issues with projected gradient descent is computing the projection of a candidate variable $x_k$.
Hence, I am trying to reduce the total number of Projection computations required for the algorithm to converge.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution?&lt;/h2&gt;
&lt;p&gt;Taking the motivation from Runge-Kutta logic, consider the following iterations where we take projections on alternate steps as shown below. (I am calling it 2-step skip projection),
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(y_{k+1}-\eta_2\nabla f(y_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;Similarly, we can consider a 3-step skip projection as well,
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
z_{k+1} = y_{k+1} - \eta_2\nabla f(y_{k+1})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(z_{k+1}-\eta_3\nabla f(z_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;We can keep increasing the number of skips. Plotting for a simple example,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_overview.png&#34; alt=&#34; Overview &#34;&gt;&lt;/p&gt;
&lt;p&gt;Here the each point in the graph represents the $error(k) = \log (f(Pr(x_k)) - f(x^*))$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_zoom.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-two-extremes&#34;&gt;The two extremes&lt;/h2&gt;
&lt;p&gt;The two extreme can be thought of as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Standard Projected gradient descent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Infinite step look ahead gradient descent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This extreme case of this skipping will be when we are projecting only after we reach the optimal point
$$\begin{equation}
y = \arg\min f(x)
\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x^* = P_{\chi}(y)
\end{equation}$$&lt;/p&gt;
&lt;p&gt;The problem with this being, the fixed point of both these cases ned not be the same and therein problem lies.&lt;/p&gt;
&lt;p&gt;But this need not be the same as the fixed point of the iterative scheme,
$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Thus the error between this two schemes is what we are seeing in the Figure 2.&lt;/p&gt;
&lt;h2 id=&#34;possibly-workaround&#34;&gt;Possibly Workaround&lt;/h2&gt;
&lt;p&gt;We can make $\eta$(stepsize) step dependent like $\frac{1}{k}$ which follows $\sum \eta_k = \infty$ and $\sum \eta^2 &amp;lt; \infty$ which will again ensure that $error \rightarrow 0$ as $k \rightarrow \infty$.\&lt;br&gt;
Using the same logic, I simulated it with the same functions as  previous plots. Results are as follows,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_decay.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;drawbacks&#34;&gt;Drawbacks&lt;/h2&gt;
&lt;p&gt;Even if this approach shows some change in the number of projections required, but this change wont be significant enough for a factor reduction.
There are no significant changes in the convergence rate of this new proposed algorithm, and hence I dont think there is much hope for the idea to form a good enough innovation.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic look over Chebyshev Polynomials</title>
      <link>https://parththaker.github.io/post/chebyshev/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/chebyshev/</guid>
      <description>&lt;p&gt;Though I myself am doubtful about a lot of explanations here, I&amp;rsquo;ll remember and explain whatever I can.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first start with the basics, The chebyshev polynomial of degree $n$ is denoted by $T_n(x)$ and given by the formula,&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}T_n(x) = cos(n cos^{-1}(x))\end{equation}$$&lt;/p&gt;
&lt;p&gt;They form the recursive relation given by,
$$T_n(x) = cos(n cos^{-1}(x))$$&lt;/p&gt;
&lt;p&gt;$$T_{n+1} (x) = 2 x T_n(x) - T_{n-1}$$&lt;/p&gt;
&lt;p&gt;Extremum of Chebyshev polynomial $T_n$ are given in the form of,&lt;/p&gt;
&lt;p&gt;$$ x_k = cos\left(\frac{2k-1}{2n}\pi\right),\ \ \ \ \ k=1,2,\dots, n$$&lt;/p&gt;
&lt;p&gt;They look something like this&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/chebyshev.png&#34; alt=&#34; Chebyshev &#34;&gt;&lt;/p&gt;
&lt;p&gt;The extremum of  Chebyshev polynomials are distributed over the entire range of the approximation and have alternating values of plus or minus unity. These characteristics make Chebyshev polynomials an ideal basis for approximating functions. It&amp;rsquo;s a well known fact that Chebyshev function approximation is as good as it gets.&lt;/p&gt;
&lt;p&gt;It is known that the mean squared error for a function fitting is minimum for a minimax function. But evaluation of a minimax polynomial is computationally expensive. Chebyshev polynomial fitting is quite close to minimax polynomial fitting and is computationally much cheaper.&lt;/p&gt;
&lt;p&gt;$$f= \sum_{i=0}^M c_iT_i$$&lt;/p&gt;
&lt;p&gt;where $T_i$ is the Chebyshev polynomial of order starting from i. {$c_i$} indicate the chebyshev coefficients.&lt;/p&gt;
&lt;p&gt;Normally the chebyshev coefficients fall off exponentially with the index $i$. Partly I think it was because the chebyshev polynomials of order $i$ had a $2^i$ term as the coefficient of $x^i$. But the prof said that it was because of that the chebyshev coefficients dropping rate is directly proportional to the Region Of Convergence.&lt;/p&gt;
&lt;p&gt;In order to apply stable Chebyshev fitting the Taylor expansion of the function should have a bounded reminder term. Other than this Chebyshev can normally shrink the number of terms used for in polynomial fitting with a bounded error which is normally the sum of magnitudes of all the coefficients of the neglected terms. This is even possible because the max value of the polynomials is +/- 1 and because the chebyshev coefficients die out much easily this error is bound with a very small value when you neglect the higher order terms.&lt;/p&gt;
&lt;p&gt;A very good thing about chebyshev polynomials i liked was the intuition or a way to look at it. It can be looked as though it takes the points on the real line from -1 to 1 and maps them to a unit circle of radius 1 and center 0 and then back to the real line. If u think about it  this mapping sort of elongates the function and makes the curves much less steeper. Thus it can be seen as to why the Chebyshev fitting requires much less order polynomial then its counter Taylor expansion, though both are  polynomial fittings.&lt;/p&gt;
&lt;p&gt;When looked at the recurrence equation of chebyshev polynomials we have,&lt;/p&gt;
&lt;p&gt;$$T_{n+1} = 2xT_{n} -T_{n-1}$$&lt;/p&gt;
&lt;p&gt;Analyzing this equation we get that the Chebyshev polynomial derived from this can only lead to stable solutions within +1 and -1. Analysis of the roots of characteristic equations makes this even more evident.&lt;/p&gt;
&lt;p&gt;Chebyshev does better than Fourier in almost all cases except for periodic functions, where Fourier can leave Cheby in the dust. But both of them do worse for functions with non differentiable peaks or valleys within the interval of consideration. It is advised to break the functions at such peaks and then do Chebyshev fitting on separate parts for better results.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experimentation with audio sink</title>
      <link>https://parththaker.github.io/post/working-with-speaker-port/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/working-with-speaker-port/</guid>
      <description>&lt;p&gt;First I tried experimenting , first good/cool thing I found was this command :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /dev/urandom | padsp tee /dev/audio &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OR&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /dev/urandom | padsp tee /dev/dsp &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both produces white noise in speakers.&lt;/p&gt;
&lt;p&gt;On good/sad thing is it sends to the audio sink you have&amp;hellip; If the comp is on speaker mode&amp;hellip; the noise will be in your speakers&lt;/p&gt;
&lt;p&gt;If there are headphones, it will be your headphones..&lt;/p&gt;
&lt;p&gt;But on trying to do something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;somefile&amp;gt;.mp3 | padsp tee /dev/audio &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly it didn&amp;rsquo;t work. Reason : My speculation, .mp3 is a coding format and it might not have suited the format in which /dev/audio wanted it&amp;hellip;&lt;/p&gt;
&lt;p&gt;On to next cool thing (found a lot of them while tweaking things) is the program called mpg123. It is used as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpg123 &amp;lt;somefile&amp;gt;.mp3 | /dev/pcsp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;/dev/pcsp&lt;/code&gt; basically is your speaker. &lt;code&gt;mpg123&lt;/code&gt; decodes .mp3 file into raw streaming data which is piped (&amp;lsquo;|&amp;rsquo;) to speakers&lt;/p&gt;
&lt;p&gt;A good thing about speakers is that it can overlap all noises no locking system as to who can write and stuff. Nice one&amp;hellip;&lt;/p&gt;
&lt;p&gt;Ok.. So we got a way in which we can run raw data streams into speakers. Now how about capturing what is being run to the speakers. In other words snooping on the /dev/pcsp (I am not sure /dev/pcsp is the right place to snoop.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;arecord -f cd -t raw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above command starts recording whatever is heard on the speakers. Now to pipe it to a music file we use the &amp;lsquo;oggenc&amp;rsquo; package. So finally its&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;arecord -f cd -t raw | oggenc - -r -o file.ogg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a music file in the current directory which is capturing all of the sound played in your speakers.&lt;/p&gt;
&lt;p&gt;So where are we right now. We have a way to play to a speaker, we have a way to record from a speaker. Now the remaining is to make a connection.&lt;/p&gt;
&lt;p&gt;Supporting articles :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://debian-administration.org/article/145/use_and_abuse_of_pipes_with_audio_data&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This article deals with piping raw data to files of 0 byte size (FIFO files) and then playing it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://debian-administration.org/article/58/Netcat_The_TCP/IP_Swiss_army_knife&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This article deals with using &lt;code&gt;netcat&lt;/code&gt; for listening and sending data on ports.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux DC&#43;&#43; problem</title>
      <link>https://parththaker.github.io/post/caution-linux-dc/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/caution-linux-dc/</guid>
      <description>&lt;p&gt;Linux DC++ is a quite popular File sharing application used on the Linux OS. It looks something like this …&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/linuxdc.png&#34; alt=&#34;LinuxDC++&#34;&gt;&lt;/p&gt;
&lt;p&gt;One of the major benefits which I saw with Linux DC++ compared to its alternatives like Eiskalt DC++ is that Linux DC++ consumes a relatively smaller amount of RAM and doesn&amp;rsquo;t lag the system which I found to be a big drawback for Eiskalt DC++&lt;/p&gt;
&lt;p&gt;Anyway, there was one erratic behavior which I was quite irritated about in Linux DC++. At some random time on some random you are peacefully downloading stuff from DC and then suddenly the entire downloads directory is missing.&lt;/p&gt;
&lt;p&gt;You already have space constraints in your system and then the downloads folder with some very &lt;em&gt;ahem ahem&lt;/em&gt; important files just go away and worse … the space is not even empty.&lt;/p&gt;
&lt;p&gt;So I started looking up the bug, and viola found it.&lt;/p&gt;
&lt;p&gt;You can find all your stuff in folders within ~/.dc++/FileLists/. There will be a lot of folders with very weird names and all of your deleted stuff will be inside there. Just have to bring them out using the mv command.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing multiple github accounts</title>
      <link>https://parththaker.github.io/post/github-managing-multiple-accounts/</link>
      <pubDate>Sat, 01 Jul 2017 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/github-managing-multiple-accounts/</guid>
      <description>&lt;p&gt;When you are dealing with multiple github accounts you have to be careful of two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Having proper rights to pull from git&lt;/li&gt;
&lt;li&gt;Having proper rights to push to git&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It may seem as to the solution to both the problems should be the same, but they are slightly different. Lets look at both the points :&lt;/p&gt;
&lt;h2 id=&#34;having-proper-rights-to-pull-from-git&#34;&gt;Having proper rights to pull from git&lt;/h2&gt;
&lt;p&gt;When you are dealing with multiple accounts, there is 
&lt;a href=&#34;http://mherman.org/blog/2013/09/16/managing-multiple-github-accounts/#.WFKRwHV948o&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; this nice post &lt;/a&gt;
 which gives the instructions as to how to go about it.&lt;/p&gt;
&lt;p&gt;Here the basic essence of the whole procedure is that one has to be careful as to which SSH key is being associated with the github account. This is being precisely targeted in the above indicated post.&lt;/p&gt;
&lt;h2 id=&#34;having-proper-rights-to-push-to-git&#34;&gt;Having proper rights to push to git&lt;/h2&gt;
&lt;p&gt;Now the first question which comes to mind is, &amp;ldquo;Why isn&amp;rsquo;t the last setup sufficient for resolving this?&amp;quot;.&lt;/p&gt;
&lt;p&gt;That would have been the case (not requiring further complications) if only one user can work on a single SSH key. You can have a single SSH key being used by different user-names on different github project. Hence the question arises, &amp;ldquo;Which is the correct user-name for the current commit push?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Thus, in conclusion, one should set up local config setting for each project using,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git config user.email &amp;quot;accountname@domain.com&amp;quot;
git config user.name &amp;quot;username&amp;quot;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can as well use the above commands with the global flag to set it common for all the projects as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git config --global user.email &amp;quot;accountname@domain.com&amp;quot;
git config --global user.name &amp;quot;username&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will set the credentials for user.email and user.name as a backup option when the local setting are not present. However I would discourage the use of the global flag. A logical thought experiment is below:&lt;/p&gt;
&lt;p&gt;Lets consider you start a new project with a username which is different from what is set using the global flag. Now in case you forget to setup the local credentials and push your changes then the changes are being pushed with a undesirable username and email.&lt;/p&gt;
&lt;p&gt;If on the other hand, the global user.email and user.name settings are not present, git throws error during pushing that you have not setup the local settings and thus cannot push, which will remind you to setup the credentials which you want to push the changes by.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selected Topics in Constrained Optimization</title>
      <link>https://parththaker.github.io/publication/thaker-dual-thesis-2016/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-dual-thesis-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When to arrive in a congested system : Achieving Equilibrium via Learning Algorithm</title>
      <link>https://parththaker.github.io/publication/thaker-rawnet-2017/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/publication/thaker-rawnet-2017/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
