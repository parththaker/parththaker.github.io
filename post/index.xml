<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Parth Thaker</title>
    <link>https://parththaker.github.io/post/</link>
      <atom:link href="https://parththaker.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2022 Parth Thaker</copyright><lastBuildDate>Tue, 01 Sep 2020 12:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://parththaker.github.io/post/</link>
    </image>
    
    <item>
      <title>Optimal strategy for classroom behaviour</title>
      <link>https://parththaker.github.io/post/parallel-bsc/</link>
      <pubDate>Tue, 01 Sep 2020 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/parallel-bsc/</guid>
      <description>&lt;p&gt;During the course of attending Information Theory by Prof. Lalitha Sankar, I encountered the following question from Thomas &amp;amp; Clover in one of the homeworks:&lt;/p&gt;
&lt;p&gt;Calculate the capacity of the following channel with the probability transistion matrix,&lt;/p&gt;
&lt;p&gt;$$\begin{bmatrix}p &amp;amp; 1-p &amp;amp; 0 &amp;amp; 0\\ 1-p &amp;amp; p &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; q &amp;amp; 1-q\\ 0 &amp;amp; 0 &amp;amp; 1-q &amp;amp; q \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Mind&amp;rsquo;s muscle memory kicked in as the math derivation lead to the conclusion that optimal strategy is to use the channel according to the ratio: $\frac{2^{-H(p)}}{2^{-H(p)} +2^{-H(q)}}$, where $H(\alpha)$ would denote the entropy of Bernoulli($\alpha$).&lt;/p&gt;
&lt;p&gt;A few moment of stagnant staring at the problem got me thinking of a potential use case.&lt;/p&gt;
&lt;p&gt;A student is attending a lecture and the BSC channels model the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;First BSC channel&lt;/strong&gt; : Denotes the capability of the student for understanding the material taught in the lecture. Let $X$ denote the event that the student is attentive to what is being taught in the lecture or not and $Y$ model whether the student understood the concept or not.
$\mathbb{P}(Y|X)$ denotes the probability matrix of the outcomes.
In order for the channel to behave as a Bernoulli($\alpha$), we make the following assumptions:
&lt;ul&gt;
&lt;li&gt;If the student is in tune with the teaching style of the lecturer, then $p\approx 1$ (a perfect learner). $\mathbb{P}(Y=1 | X=1) \approx 1$ i.e. given complete attention, student understands it completely. On the other hand, given no attention student doesnt gain anything hence $P(Y=0 | X=0)\approx 1$&lt;/li&gt;
&lt;li&gt;If the student is completely oblivious to the lecturer&amp;rsquo;s teaching patterns and is attending the course because his grad advisor forced his hand, then we model it as a Bernoulli($\frac{1}{2}$). This is because whether the student is attentive or not, the conclusion is that the student is not understanding the course content.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!--the of the material being taught in class -- By this I mean, for a person having full knowledge of measure theory might find an introductory class on Probability a bit easy. 
Thus the person would have parameter $p\approx \frac{1}{2}$, which indicates the corresponds that channel being noisy $\leftrightarrow$ easy class. This would also mean that a look at the material taught by the prof once every 10 minutes would give him a complete picture of what is happening in class. 
--&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Second BSC channel&lt;/strong&gt; : Denotes the capability of student in alternate pursuits in classroom &amp;ndash;
&lt;ul&gt;
&lt;li&gt;Thinking of research project the student is working on (this is me)&lt;/li&gt;
&lt;li&gt;Thinking of strategies in board games (also me. 
&lt;a href=&#34;https://boardgamegeek.com/boardgame/120677/terra-mystica&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Terra Mystica&lt;/a&gt;
!!!)&lt;/li&gt;
&lt;li&gt;Chatting on phone, replying to mails, twitter ,etc.
The parameter $q$ models how clearly one can think of that other task being performed while in classroom.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we consider this model then the surprising conclusion of this homework problem comes to :&lt;/p&gt;
&lt;p&gt;&lt;em&gt;It is information theoritically suboptimal to completely focus on the content being taught in class&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is an ideal ratio in which both needs to be pursued during your classroom time. So, the next time you have are caught distracted in class you have an argument to make.&lt;/p&gt;
&lt;p&gt;Though you cant completely zone out as well, since that is also a suboptimal strategy :)&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projected gradient descent with skipping</title>
      <link>https://parththaker.github.io/post/skipping-projection/</link>
      <pubDate>Tue, 01 May 2018 12:00:00 +0000</pubDate>
      <guid>https://parththaker.github.io/post/skipping-projection/</guid>
      <description>&lt;p&gt;One of the major benefits (among million others) of attending an elite university like IIT Madras is the various avenues it provides to get a glimspe into a world which you never knew of. Pursuing research and the thought of PhD was one such world which was brought within my grasp by IITM.&lt;/p&gt;
&lt;p&gt;I was fortunate enough to attend one of the guest lecture by 
&lt;a href=&#34;http://people.cs.umass.edu/~mahadeva/Site/About_Me.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Sridhar Mahadevan&lt;/a&gt;
, University of Massachusetts, during his visit to IIT Madras.
He was going to discuss about his recent work (at that time) 
&lt;a href=&#34;https://arxiv.org/pdf/1405.6757v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent work&lt;/a&gt;
 with his student 
&lt;a href=&#34;https://imgemp.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ian Gemp&lt;/a&gt;
.
The plan of the talk (atleast to me) seemed to be : Start with variational inequalities and proceed to extragradient methods and state the results they worked out.&lt;/p&gt;
&lt;p&gt;In the middle of the seminar, he said about a curious finding which pinged my interest.
They had applied a logic based on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Runge-Kutta methods&lt;/a&gt;
 to their problem and noticed that system kept converging faster and faster as the order of R-K method was increased.
This was followed by a question by him,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;So how much acceleration can we possibly obtain by including more and more terms?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This phenomenon irked me. According to my firm belief, there is &lt;strong&gt;always&lt;/strong&gt; a trade-off. A system which just keeps providing improved one-step convergence without consequence? Something was definitely off.&lt;/p&gt;
&lt;!--
## Glimpse of Runge-kutta

Runge-Kutta methods have been used to find approximate solutions in Ordinary Differential Equations.

A General Runge-Kutta Gradient descent update looks like,

$$\begin{equation}k_1 = \alpha  \nabla F(x_k)\end{equation}$$
$$\begin{equation}k_2 = \alpha \nabla F(x_k - a\_{21}k_1)\end{equation}$$
$$\begin{equation}k_3 = \alpha \nabla F(x_k - a\_{31}k_1 - a\_{32}k_2)\end{equation}$$

So for general term,

$$\begin{equation}k_s = \alpha \nabla F(x_k - a\_{s1}k_1 - a\_{s2}k_2 - \dots - a\_{s,s-1}k\_{s-1})\end{equation}$$

The next iterate $x_k$ in the sequence of the algorithm as,

$$\begin{equation}x\_{k+1} = x\_{k} - \sum\_{i=1}^sb_i k_i\end{equation}$$
--&gt;
&lt;p&gt;My lifelong passion has been the theory behind optimization methods. Hence, I tried replicating similar situation (some would argue its exactly the same on some plane of thought) in constrained optimization using projected gradient descent.&lt;/p&gt;
&lt;p&gt;Before diving deep into the exact question, a brief primer on the setup,&lt;/p&gt;
&lt;h2 id=&#34;constrained-convex-optimization&#34;&gt;Constrained Convex Optimization&lt;/h2&gt;
&lt;p&gt;A simple constrained optimization problem of optimizing a convex function $f(x)$ over the set $x\in\chi$ can be stated as,
$$\begin{equation}
\min\ \ \ f(x)\ \ \ \  \text{over}\ \ \ \  x\in \chi
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where assume that $f(x)$ is a doubly differentiable.&lt;/p&gt;
&lt;p&gt;One of the go-to approach to solving the above problem is through projected gradient descent.
The next iterate $x_{k+1}$ can we written as,&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where the projection operator $P_{\chi}(x)$ is defined as,
$$\begin{equation}
P_{\chi}(x) = \arg\min_{y\in \chi}||x-y||^2
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Taking the que from Prof. Sridhar, consider the following iterations where we take projections on alternate steps as shown below. (I am calling it 2-step skip projection),
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(y_{k+1}-\eta_2\nabla f(y_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;Similarly, we can consider a 3-step skip projection as well,
$$\begin{equation}
y_{k+1} = x_{k} - \eta_1\nabla f(x_{k})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
z_{k+1} = y_{k+1} - \eta_2\nabla f(y_{k+1})\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(z_{k+1}-\eta_3\nabla f(z_{k+1}))\end{equation}$$&lt;/p&gt;
&lt;p&gt;Further, we can keep increasing the number of skips to obtain any $k$-skip projected gradient descent.&lt;/p&gt;
&lt;p&gt;Below is the error plot of these $k$-skip projected gradient descent for a simple example, where $error(k) = \log (f(Pr(x_k)) - f(x^*))$,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_overview.png&#34; alt=&#34; Overview &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;initial-thoughts&#34;&gt;Initial thoughts&lt;/h2&gt;
&lt;p&gt;On first look, this experimental evidence confirms Prof.Sridhar&amp;rsquo;s claims!! More skipping just provides better one-step convergence.&lt;/p&gt;
&lt;p&gt;But where is the trade-off? Lets look closely as to &lt;em&gt;where&lt;/em&gt; the different $k$-skip algorithms are converging.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_zoom.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;p&gt;Note after enlarging the tail of the graphs, we can notice at all the distinct $k$-skip algorithms have &lt;em&gt;distinct&lt;/em&gt; convergence point.
And moreover, the more order of skip -&amp;gt; the farther away from the optimal solution (here optimal solution is the convergence point of standard projected gradient descent)&lt;/p&gt;
&lt;p&gt;Why are we observing this?&lt;/p&gt;
&lt;h2 id=&#34;the-two-extreme-behaviour&#34;&gt;The two extreme behaviour&lt;/h2&gt;
&lt;p&gt;We can consider the following high level view. Considering skipping with projected gradient descent, there are two extreme behaviour which can be thought of as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Standard Projected gradient descent&lt;/strong&gt; :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{equation}
x_{k+1} = P_{\chi}(x_{k} - \eta\nabla f(x_k))
\end{equation}$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Infinite-step look-ahead gradient descent projection&lt;/strong&gt; :
The extreme case of $k$-skip algorithms will be when we are projecting only after we reach the optimal point
$$\begin{equation}
y = \arg\min f(x)
\end{equation}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{equation}
x^* = P_{\chi}(y)
\end{equation}$$
As noted in the sample simulations, the issue here is the difference in the fixed points in both these cases.&lt;/p&gt;
&lt;p&gt;This provides a good intuition for the behaviour witnessed in tail-end of Figure 2.&lt;/p&gt;
&lt;h2 id=&#34;possibly-workaround&#34;&gt;Possibly Workaround&lt;/h2&gt;
&lt;p&gt;One easy and quick workaround to make sure that all the $k$-skip algorithms converge to the &lt;em&gt;same&lt;/em&gt; point is to have decaying stepsizes i.e., we can make step-size (\eta) decay as $\frac{1}{t}$ which follows $\sum \eta_t = \infty$ and $\sum \eta_t^2 &amp;lt; \infty$.&lt;/p&gt;
&lt;p&gt;Simulating the same setup with the new step-size routine, we obtain the following tail behaviour,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/sk_proj_decay.png&#34; alt=&#34; Zoom &#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;benefits&#34;&gt;Benefits&lt;/h2&gt;
&lt;p&gt;In constrained optimization, one of the major computation issues concerning projected gradient descent is the projection step. Using the $k$-skip projected gradient descent reduces the total number of projections required by a multiplicative factor.&lt;/p&gt;
&lt;!--
## Drawbacks

Even if this approach shows some change in the number of projections required, but this change wont be significant enough for a factor reduction.
There are no significant changes in the convergence rate of this new proposed algorithm, and hence I dont think there is much hope for the idea to form a good enough innovation.
--&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2017 
&lt;a href=&#34;https://parththaker.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parth Thaker&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;Released under the 
&lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt;
 license.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
